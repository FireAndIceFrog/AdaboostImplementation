{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import KFold\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adabooster:\n",
    "    def __init__(self,debug = False):\n",
    "        self.debug = debug\n",
    "        self.marginDF = None\n",
    "        return\n",
    "    def saveClassifier(self, file=\"Adabooster_single.csv\"):\n",
    "        self.classifier_df = pd.DataFrame(np.concatenate((self.h,np.expand_dims(self.alpha, axis=1)), axis = 1) , columns = [\"Threshold\",\"Feature\",\"Direction\",\"Alpha\"])\n",
    "        self.classifier_df.to_csv(\"Adabooster_single.csv\")\n",
    "    def loadClassifier(self,file = \"Adabooster_single.csv\"):\n",
    "        self.classifier_df = pd.read_csv(file,index_col = 0)\n",
    "    def reset_params(self,Tt, datat):\n",
    "        'Reset the parameters for the booster round'\n",
    "        \n",
    "        self.T = Tt\n",
    "        self.h  = np.zeros([self.T, 3], dtype=np.float64)\n",
    "        self.alpha  = np.zeros(self.T, dtype=np.float64)\n",
    "        self.err  = np.ones(self.T, dtype=np.float64) * np.inf\n",
    "        self.weight  = np.ones(datat.shape[0], dtype=np.float64) / datat.shape[0]\n",
    "        self.dim = datat.shape[1]\n",
    "    def calculate_decision_stump(self,data, feature, weight, label):\n",
    "        'Calculate the desicion stump for the next booster round'\n",
    "        Tp=np.float64(0); #T+ total sum of positive examples weights\n",
    "        Tn=np.float64(0) #T- total sum of negative examples weights\n",
    "        Sp=np.float64(0) #S+ sum of positive weights below the cuurent threshold\n",
    "        Sn=np.float64(0) #S- sum of negative weights below the current threshold\n",
    "        error1=np.float64(0)\n",
    "        error2=np.float64(0)\n",
    "        min_error=np.float64(2.0) \n",
    "        min_thresh=np.float64(0) \n",
    "        direction=1\n",
    "\n",
    "        y = np.zeros(data.shape[0], dtype=np.int64)\n",
    "\n",
    "        #get all positive weights    \n",
    "        temp  = (label == 1)\n",
    "        temp = np.int64(temp)\n",
    "        Tp = np.sum(temp * weight)\n",
    "\n",
    "        #get all negative weights  \n",
    "        temp  = (label == -1)\n",
    "        temp = np.int64(temp)\n",
    "        Tn = np.sum(temp * weight)\n",
    "\n",
    "        #sort feature values\n",
    "        sorted_labels = data[:, feature].argsort()\n",
    "        sorted_vector =  data[sorted_labels]\n",
    "\n",
    "        length = len(sorted_vector)\n",
    "        for i in range(length):\n",
    "\n",
    "            #RIGHT DIRECTION THRESHOLD\n",
    "            #error1 is the sum of positives up to that point + total negatives minus the sum of negatives so far\n",
    "            error1 = Sp + (Tn - Sn) \n",
    "            if label[sorted_labels[i]] == -1 : \n",
    "                Sn = Sn +  weight[sorted_labels[i]]\n",
    "            else :\n",
    "                Sp = Sp + weight[sorted_labels[i]]\n",
    "\n",
    "            #LEFT DIRECTION THRESHOLD\n",
    "            error2 = Sn + (Tp - Sp) \n",
    "\n",
    "            if(min_error > error1) :\n",
    "                min_error = error1\n",
    "                min_thresh = sorted_vector[i, feature]\n",
    "                direction = 1\n",
    "            if(min_error > error2) :\n",
    "                min_error = error2\n",
    "                min_thresh = sorted_vector[i, feature]\n",
    "                direction = -1           \n",
    "\n",
    "        return min_thresh, direction, min_error\n",
    "    def calculate_alpha(self,weighted_error):    \n",
    "        #========================\n",
    "        #YOUR CODE HERE\n",
    "        #========================\n",
    "        \n",
    "        return  0.5 * np.log( (1.0 - weighted_error) / weighted_error )\n",
    "    def classify_dataset_against_weak_classifier(self,x, thresh, direction):\n",
    "        classification = np.zeros(len(x))\n",
    "\n",
    "        #classifiy all samples based on the last feature\n",
    "        #get actual classification\n",
    "        for i in range(len(x)):\n",
    "            #========================\n",
    "            #YOUR CODE HERE\n",
    "            #========================\n",
    "            if direction == -1:\n",
    "                if x[i] < thresh: classification[i] = 1\n",
    "                else : classification[i] = -1\n",
    "            else:\n",
    "                if x[i] < thresh: classification[i] = -1\n",
    "                else : classification[i] = 1    \n",
    "\n",
    "\n",
    "        return classification \n",
    "    def update_weights(self,weight, alpha, classification, label):\n",
    "\n",
    "        for i in range(len(weight)):\n",
    "            #========================\n",
    "            #YOUR CODE HERE\n",
    "            #========================\n",
    "            weight[i] =  weight[i] * np.exp( -1.0 * alpha * classification[i] * label[i] ) \n",
    "\n",
    "        return weight\n",
    "    def normalise_weights(self,weight):\n",
    "\n",
    "        #========================\n",
    "        #YOUR CODE HERE\n",
    "        #========================\n",
    "        weight = weight / np.sum(weight)\n",
    "\n",
    "        return weight \n",
    "    def fit(self, x , label ):\n",
    "        # This is for future use\n",
    "        T = self.T\n",
    "        h = self.h\n",
    "        alpha = self.alpha\n",
    "        err = self.err\n",
    "        weight = self.weight\n",
    "    \n",
    "        #0 - for each boosting round\n",
    "        for t in range(T): \n",
    "            #1 - iterate through every feature  \n",
    "\n",
    "                \n",
    "            for feature in range(self.dim): \n",
    "                weighted_error = np.float64(0)\n",
    "\n",
    "                #========================\n",
    "                #2 - GENERATE A DECISION STUMP FOR A FEATURE\n",
    "                #YOUR CODE HERE\n",
    "                #========================\n",
    "                threshold, sign, weighted_error = self.calculate_decision_stump(x , feature, weight, label)\n",
    "                #========================\n",
    "                #3 - KEEP TRACK OF THE FEATURE WITH THE LOWEST WEIGHTED ERROR\n",
    "                #YOUR CODE HERE\n",
    "                #========================        \n",
    "                if weighted_error < err[t] :\n",
    "                    err[t] = weighted_error\n",
    "                    h[t][0] = threshold\n",
    "                    h[t][1] = feature\n",
    "                    h[t][2] = sign\n",
    "\n",
    "\n",
    "            #========================\n",
    "            #4 - CALCULATE ALPHA FOR BOOSTING ROUND t\n",
    "            #YOUR CODE HERE\n",
    "            #========================            \n",
    "            alpha[t] = self.calculate_alpha(err[t])\n",
    "\n",
    "            #========================\n",
    "            #5 - CLASSIFY ALL SAMPLES BASED ON THE SELECTED FEATURE FOR BOOSTING ROUND t\n",
    "            #YOUR CODE HERE\n",
    "            #======================== \n",
    "            #print(x[:, int(h[t][1]) ])\n",
    "            classification = self.classify_dataset_against_weak_classifier(x[:, int(h[t][1]) ], h[t][0], h[t][2] )\n",
    "\n",
    "            #========================\n",
    "            #6 - UPDATE WEIGHTS BASED ON THE CORRECTNESS OF THE CLASSIFICATION\n",
    "            #YOUR CODE HERE\n",
    "            #========================   \n",
    "            weight = self.update_weights(weight, alpha[t], classification, label)\n",
    "\n",
    "            #========================\n",
    "            #7 - NORMALISE REASSIGNED WEIGHTS\n",
    "            #YOUR CODE HERE\n",
    "            #========================  \n",
    "            weight = self.normalise_weights(weight )\n",
    "\n",
    "            #--------------------------------------------\n",
    "            #BOOSTING ALGORITHM DONE\n",
    "            #--------------------------------------------\n",
    "            if (self.debug):\n",
    "               print(\"Round \",t, \" Done!\")\n",
    "    def classify_sample(self,xi ):\n",
    "        boost_classif = self.classifier_df\n",
    "        boost_classif = boost_classif.values\n",
    "        classification_sum = np.float64(0)\n",
    "\n",
    "        for thresh, feat, sign, alpha in boost_classif:\n",
    "            feat = np.int64(feat)\n",
    "            temp = np.float64(0)\n",
    "            if(sign == 1):\n",
    "                temp = (xi[feat] >= thresh)\n",
    "            else:\n",
    "                temp =  (xi[feat]< thresh)\n",
    "\n",
    "            temp = alpha*(-1 if temp == 0 else temp)\n",
    "\n",
    "\n",
    "            classification_sum = classification_sum + temp\n",
    "\n",
    "\n",
    "        if classification_sum >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    def score(self,test_x,test_y):\n",
    "        results = []\n",
    "        for i in range(len(test_x)):\n",
    "            results.append(self.classify_sample(test_x[i]))\n",
    "        results = np.array(results)\n",
    "        return  len(results[results == test_y])/len(results*100)\n",
    "    def sum_classifier_votes_for_each_sample(self, dataset, df):\n",
    "        classifier_df = self.classifier_df\n",
    "        for i in range(len(dataset)):\n",
    "            classification_sum = np.float64(0)\n",
    "            neg_votes = np.float64(0)\n",
    "            pos_votes = np.float64(0)\n",
    "            for idx, thresh, feat, sign, alpha in classifier_df.itertuples():\n",
    "                #========================\n",
    "                #YOUR CODE HERE\n",
    "                #========================  \n",
    "                feat = np.int64(feat)\n",
    "                temp = np.float64(0)\n",
    "                if(sign == 1):\n",
    "                    temp = (dataset[i][feat] >= thresh)\n",
    "                else:\n",
    "                    temp =  (dataset[i][feat]< thresh)\n",
    "\n",
    "                temp = alpha*(-1 if temp == 0 else temp)\n",
    "                if temp < 0:\n",
    "                    neg_votes = neg_votes+temp\n",
    "                else : \n",
    "                    pos_votes = pos_votes +temp\n",
    "\n",
    "\n",
    "\n",
    "                classification_sum = classification_sum + temp\n",
    "\n",
    "\n",
    "            #========================\n",
    "            #YOUR CODE HERE\n",
    "            #========================  \n",
    "            df['sum_alpha'].iloc[i] = classification_sum\n",
    "            df['pos_votes'].iloc[i] = pos_votes\n",
    "            df['neg_votes'].iloc[i] = neg_votes\n",
    "\n",
    "        return df\n",
    "    def margin_calculation(self,sign, pos, neg, tot_votes):\n",
    "        #========================\n",
    "        #YOUR CODE HERE\n",
    "        #========================   \n",
    "        margin = (pos/tot_votes if sign>0 else neg/tot_votes)\n",
    "\n",
    "        return margin\n",
    "    def margin_calculation_for_training_samples(self, sign, pos, neg, tot_votes ):  \n",
    "        if np.sign(sign) < 0:\n",
    "            return np.abs(neg) / tot_votes, -1\n",
    "        else:\n",
    "            return pos / tot_votes, 1\n",
    "    def sign_of_margin(self, margin, classification, true_class_label):\n",
    "        #========================\n",
    "        #YOUR CODE HERE\n",
    "        #========================      \n",
    "        return (margin if (classification == true_class_label) else -margin)\n",
    "    def calculate_margins(self,x,label):\n",
    "        testing_set_df = pd.DataFrame(x)\n",
    "        testing_set_df['sum_alpha'] = 0 \n",
    "        testing_set_df['pos_votes'] = 0 \n",
    "        testing_set_df['neg_votes'] = 0 \n",
    "\n",
    "        testing_set_df = self.sum_classifier_votes_for_each_sample(x, testing_set_df)\n",
    "        total_alpha_votes = np.sum(self.classifier_df.Alpha)\n",
    "        testing_set_df['classification'] = 0\n",
    "        testing_set_df['margin'] = 0\n",
    "        testing_set_df['total_alpha_votes'] = total_alpha_votes\n",
    "        \n",
    "        result = testing_set_df[['sum_alpha','pos_votes','neg_votes','total_alpha_votes']].apply(lambda x: self.margin_calculation_for_training_samples(*x), axis=1)\n",
    "        testing_set_df['margin'] = result.apply(lambda x: x[0])\n",
    "        testing_set_df['classification'] = result.apply(lambda x: x[1])\n",
    "        testing_set_df['true_class_label'] = label\n",
    "        \n",
    "        testing_set_df['sign_of_margin'] = testing_set_df[['margin', 'classification', 'true_class_label']].apply(lambda x: self.sign_of_margin(*x), axis=1)\n",
    "        self.marginDF = testing_set_df[['sign_of_margin']]\n",
    "        return self.marginDF\n",
    "    def plotMargins(self):\n",
    "        margin_30 = self.marginDF[[\"sign_of_margin\"]]\n",
    "        sns.kdeplot(margin_30.sign_of_margin, cumulative=True, label='classifier size 30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the dataset and k-fold it\n",
    "spamDF = pd.read_csv(\"spambase.data\", header = None,index_col=False)\n",
    "spamDF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NAN values\n",
    "for col in spamDF.columns:\n",
    "    spamDF[col].fillna(spamDF[col].median(),inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = spamDF[list(range(len(spamDF.columns)-1))].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Identifier:  60.59552271245382\n",
      "1 Identifier:  39.404477287546186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4601,)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = spamDF[len(spamDF.columns)-1].apply(lambda a: 1 if a==1 else -1).values\n",
    "print(\"0 Identifier: \",len(Y[Y==-1])/len(Y)*100)\n",
    "print(\"1 Identifier: \",len(Y[Y==1])/len(Y)*100)\n",
    "Y.shape\n",
    "# Clearly the data is skewed, we will need to check that this is the same in the final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n",
      "[-1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Define the folder\n",
    "kf = KFold(n_splits=3)\n",
    "# Create the testing sets\n",
    "Y_correct = Y[Y==1]\n",
    "Y_incorrect = Y[Y==-1]\n",
    "X_correct = X[Y==1]\n",
    "X_incorrect = X[Y==-1]\n",
    "\n",
    "print(Y_correct[:3])\n",
    "print(Y_incorrect[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x = []\n",
    "test_x = []\n",
    "train_y = []\n",
    "test_y = []\n",
    "\n",
    "# Add the correct rows\n",
    "for train_index, test_index in kf.split(Y_correct):\n",
    "    X_train, X_test = X_correct[train_index], X_correct[test_index]\n",
    "    y_train, y_test = Y_correct[train_index], Y_correct[test_index]\n",
    "    \n",
    "    train_x.append(X_train)\n",
    "    test_x.append(X_test)\n",
    "    train_y.append(y_train)\n",
    "    test_y.append(y_test)\n",
    "#     Add the incorrect rows\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(Y_incorrect):\n",
    "    X_train, X_test = X_incorrect[train_index], X_incorrect[test_index]\n",
    "    y_train, y_test = Y_incorrect[train_index], Y_incorrect[test_index]\n",
    "    \n",
    "    train_x[i] = np.append(train_x[i], X_train, axis=0)   \n",
    "    test_x[i] = np.append(test_x[i],X_test, axis=0)\n",
    "    train_y[i] = np.append(train_y[i],y_train, axis=0)\n",
    "    test_y[i]= np.append(test_y[i],y_test, axis=0)\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3066, 57)\n",
      "(3068, 57)\n",
      "(3068, 57)\n",
      "3066\n"
     ]
    }
   ],
   "source": [
    "min_rows = 1000000000\n",
    "for i in range(len(train_x)):\n",
    "    if train_x[i].shape[0] < min_rows:\n",
    "        min_rows = train_x[i].shape[0]\n",
    "    print(train_x[i].shape)\n",
    "print(min_rows)\n",
    "for i in range(len(train_x)):\n",
    "    train_x[i] = train_x[i][:min_rows]\n",
    "    train_y[i] = train_y[i][:min_rows]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = adabooster(False)\n",
    "# Create a booster and initialize it with 30 rounds\n",
    "booster.reset_params(30,train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.647557003257329\n",
      "0.7045009784735812\n",
      "0.6829745596868885\n"
     ]
    }
   ],
   "source": [
    "# If a booster already exists, dont worry about classifying (for time constraint)\n",
    "import os.path\n",
    "from os import path\n",
    "boosterFile = \"Adabooster_\"\n",
    "for i in range(len(train_x)):\n",
    "    booster.fit(train_x[i],train_y[i])\n",
    "    booster.saveClassifier(boosterFile+str(i)+\".csv\")\n",
    "    score = booster.score(test_x[i],test_y[i])\n",
    "    print(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
